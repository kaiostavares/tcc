Este trabalho investiga a aplicação de Modelos de Linguagem de Larga Escala (LLMs) como suporte à verificação automatizada da conformidade entre requisitos de software e sua implementação no código-fonte. Parte-se do reconhecimento de que a rastreabilidade entre especificações e artefatos de software é uma atividade essencial, porém propensa a falhas, omissões e retrabalho quando executada manualmente. Para tanto, são explorados fundamentos como vetorização semântica, janelas de contexto, engenharia de prompts e princípios de aprendizado de máquina aplicados ao domínio da engenharia de software.

A proposta foi validada por meio da análise de um sistema real, considerando dois módulos: um centrado em \textit{backend} e outro que envolve integração entre \textit{backend} e \textit{frontend}. Foram desenvolvidos prompts de auditoria capazes de orientar os modelos na inspeção automatizada dos arquivos do repositório, avaliando a presença ou ausência de implementações condizentes com os requisitos especificados. As saídas geradas pelos LLMs foram confrontadas com avaliações humanas e analisadas segundo métricas como acurácia simples, acurácia estrita, taxa de omissão, precisão, recall e f1-score.

Os resultados indicaram desempenho expressivo dos modelos, com acurácia estrita superior a 90\% em determinados cenários, evidenciando a viabilidade da abordagem como apoio à análise de conformidade. Apesar dos avanços, a solução apresenta limitações, como a dependência da janela de contexto e a necessidade de seleção criteriosa dos arquivos inspecionados. Como trabalho futuro, propõe-se a integração com agentes autônomos baseados no protocolo MCP, visando a automatização de ações corretivas, bem como a adoção de técnicas de recuperação aumentada por geração (RAG) para mitigar restrições impostas por repositórios extensos.
