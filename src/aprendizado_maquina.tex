\xchapter{Aprendizado de Máquina}{}

O Aprendizado de Máquina (AM) é caracterizado por ser a área voltada ao desenvolvimento de algoritmos capazes de identificar padrões em dados e, a partir deles, realizar previsões ou decisões. Segundo \citeonline{jelodar2025}, essa abordagem tem se consolidado como fundamental para tarefas de análise de código, processamento de linguagem natural e suporte a sistemas de recomendação, dado seu potencial de generalização a partir de grandes volumes de dados.

Tradicionalmente, o AM é dividido em três paradigmas: aprendizado supervisionado, não supervisionado e por reforço. O primeiro baseia-se em conjuntos de dados rotulados, permitindo que o modelo aprenda a relação entre entradas e saídas conhecidas. Em contrapartida, o aprendizado não supervisionado busca descobrir estruturas ocultas ou agrupamentos em dados não rotulados, sendo amplamente aplicado em técnicas de clusterização e redução de dimensionalidade. O aprendizado por reforço, por sua vez, concentra-se na interação de um agente com um ambiente, onde decisões são ajustadas com base em recompensas ou penalidades recebidas.

Nesse contexto, no campo do Processamento de Linguagem Natural, o Aprendizado de Máquina tem desempenhado papel essencial para a evolução de modelos cada vez mais robustos. O advento da arquitetura Transformer, proposta por \citeonline{vaswani2017}, representou um marco nesse cenário, ao introduzir mecanismos de atenção capazes de capturar dependências de longo alcance em sequências textuais. Essa inovação viabilizou a criação dos LLMs, que combinam técnicas de aprendizado profundo com grandes volumes de dados textuais para realizar tarefas como tradução automática, sumarização, análise de sentimentos e verificação semântica.

\section{Redes Neurais e Deep Learning}

As Redes Neurais Artificiais (RNAs) segundo \citeonline{choi2020}  constituem uma classe de algoritmos de aprendizado de máquina inspirada na estrutura e no funcionamento do cérebro humano. Essencialmente, uma rede neural é uma coleção de neurônios e conexões entre eles. Um neurônio pode ser entendido como uma função matemática que recebe um conjunto de entradas, realiza uma operação sobre elas e produz uma única saída. As conexões funcionam como canais que ligam a saída de um neurônio à entrada de outro, e cada conexão possui um peso que determina a força ou a importância do sinal transmitido. É através do ajuste desses pesos que a rede aprende.

Uma das arquiteturas fundamentais de uma rede neural é o Perceptron, introduzido por Rosenblatt (1958) como um modelo probabilístico para armazenamento e organização de informação no cérebro. \citeonline{asimov2017} explica que o Perceptron é um algoritmo que recebe um conjunto de entradas (features) e busca encontrar uma linha, plano ou hiperplano capaz de separar os dados em diferentes classes. Quando múltiplos perceptrons são interligados em camadas, a estrutura passa a ser denominada Rede Neural Artificial ou Perceptron de Múltiplas Camadas (MLP).

Em uma arquitetura \textit{Feedforward} (FFNN), os neurônios são organizados em uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída. A informação flui estritamente em uma direção, da entrada para a saída, sem ciclos ou retroalimentação.\citeonline{choi2020} nos mostra que para que a rede possa aprender padrões não lineares complexos, os neurônios nas camadas ocultas e de saída aplicam uma função de ativação não linear, como a sigmoide ou a Unidade Linear Retificada (ReLU), à soma ponderada de suas entradas.

Esse processo de treinamento é tipicamente realizado por meio do algoritmo de \textit{backpropagation}. A rede recebe um conjunto de dados de treinamento e, para cada exemplo, sua saída é comparada com a saída desejada (rótulo) através de uma função de perda (ou custo). O algoritmo então calcula o gradiente dessa função de perda em relação a cada peso na rede e ajusta os pesos na direção que minimiza o erro. Esse processo é repetido iterativamente para todo o conjunto de dados.

O conceito de Aprendizado Profundo (\textit{Deep Learning}) refere-se ao uso de redes neurais com um número substancial de camadas ocultas, frequentemente chamadas de Redes Neurais Profundas (DNNs). Essa profundidade permite que a rede aprenda uma hierarquia de características, onde as camadas iniciais detectam padrões simples e as camadas subsequentes combinam essas características para reconhecer conceitos mais complexos e abstratos. Um dos desafios históricos do aprendizado profundo foi a dificuldade de treinar redes com muitas camadas. Para contornar essa dificuldade, uma solução que impulsionou a área foi o pré-treinamento guloso camada a camada (\textit{greedy layer-wise pre-training}), onde cada camada é treinada de forma não supervisionada para aprender uma representação da saída da camada anterior.\citeonline{bengio2007} mostra que após o pré-treinamento de todas as camadas, a rede inteira passa por um ajuste fino supervisionado. Isso é reforçado por \citeonline{he2016}, que embora o aumento da profundidade seja crucial para o poder representacional do modelo, ele também introduz desafios como o problema da degradação, onde adicionar mais camadas a uma rede já profunda pode levar a um erro de treinamento maior.

\section{Principais Arquiteturas de Redes Neurais}

 A pesquisa em redes neurais levou ao desenvolvimento de uma ampla variedade de arquiteturas especializadas, cada uma voltada para tipos específicos de dados e tarefas computacionais. Embora frequentemente apresentadas como distintas, muitas dessas arquiteturas compartilham fundamentos conceituais comuns, adaptando-se conforme as necessidades do problema a ser resolvido. A Figura~\ref{fig:nnzoo}, localizada ao final desta seção,  fornece uma visão abrangente da diversidade arquitetônica no campo das redes neurais, ilustrando suas interconexões e áreas de aplicação.

Entre as arquiteturas fundamentais estão as redes neurais do tipo \textit{feedforward} (FFNNs), que representam a forma mais simples de rede. Nessas estruturas, os dados fluem unidirecionalmente da camada de entrada até a de saída, sem formar ciclos ou conexões recorrentes. Apesar da simplicidade, as FFNNs são modelos universais de aproximação de funções e servem como base conceitual para arquiteturas mais avançadas \citeonline{asimov2017}. Em contraste, as redes neurais recorrentes (RNNs) introduzem conexões cíclicas, permitindo que a saída de um neurônio em um instante de tempo seja retroalimentada como entrada em etapas subsequentes. Essa característica descritas por \citeonline{elman1990, asimov2017} confere às RNNs uma forma de “memória”, o que as torna particularmente adequadas para lidar com sequências temporais, como texto, sinais ou séries temporais.

Dentre as variações das RNNs, destacam-se as arquiteturas \textit{Long Short-Term Memory} (LSTM), projetadas para mitigar os problemas de desaparecimento e explosão de gradientes em sequências longas. \citeonline{hochreiter1997} explicam que as LSTMs introduzem mecanismos internos como portões de entrada, esquecimento e saída, além de uma célula de memória que permite o controle refinado do fluxo de informações ao longo do tempo. Outra alternativa popular são as \textit{Gated Recurrent Units} (GRUs), \citeonline{chung2014} explica que estas simplificam a estrutura das LSTMs ao empregar apenas dois portões de atualização e de reinicialização, reduzindo o custo computacional sem comprometer significativamente o desempenho em diversas tarefas.

Para além das arquiteturas voltadas a dados sequenciais, os autoencoders (AEs) representam uma classe importante de redes voltadas ao aprendizado não supervisionado. Essas redes buscam aprender representações latentes comprimidas dos dados por meio de uma estrutura de codificação e decodificação. Uma variação notável são os \textit{Variational Autoencoders} (VAEs), \citeonline{asimov2017, kingma2013} abordam em suas pesquisas que essa estrutura introduz uma abordagem probabilística ao aprendizado de representações, modelando os dados como distribuições e permitindo capacidades generativas.

Em tarefas de visão computacional, as redes neurais convolucionais (CNNs) tornaram-se padrão. \citeonline{asimov2017, Ranzato1998} explicam que essas redes aplicam filtros convolucionais sobre os dados de entrada, geralmente imagens, para extrair características espaciais como bordas, padrões e texturas. Em seguida, utilizam camadas de \textit{pooling} para reduzir a dimensionalidade e aumentar a robustez a pequenas variações no posicionamento espacial. No campo do aprendizado generativo, as redes generativas adversariais (GANs) se destacam por sua abordagem baseada em competição entre dois modelos: um gerador, que tenta produzir dados sintéticos realistas, e um discriminador, que busca distinguir entre dados reais e artificiais. Essa dinâmica adversarial segundo \citeonline{goodfellow2014}  conduz ao aprimoramento contínuo do gerador, resultando em amostras altamente realistas.

Por fim, arquiteturas muito profundas, como as Redes Residuais Profundas (ResNets), foram criadas para superar as dificuldades do treinamento de redes com muitas camadas. As ResNets como explica \citeonline{he2016} introduzem conexões de atalho que permitem que a informação flua entre camadas distantes, facilitando o aprendizado de funções residuais. Esse mecanismo tem se mostrado eficaz para mitigar degradação de desempenho em redes profundas, possibilitando a construção de modelos com centenas de camadas.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{imgs/neural_networks.jpeg}
  \caption{Panorama de arquiteturas de redes neurais \cite{asimov2017}}
  \label{fig:nnzoo}
\end{figure}

\FloatBarrier
\section{\textit{Large Language Models}}

O funcionamento dos \textit{Large Language Models} envolve, inicialmente, uma fase de pré-treinamento sobre corpora textuais extensos e diversos. Nessa etapa, o modelo aprende representações estatísticas das palavras e suas relações contextuais. Subsequentemente, o modelo pode ser refinado por meio de técnicas de ajuste fino (\textit{fine-tuning}) para tarefas ou domínios específicos, como medicina, direito ou engenharia de software. Essa adaptação permite que o modelo adquira conhecimento especializado e aprimore seu desempenho em aplicações particulares \cite{jelodar2025, ouyang2023}.

Uma das características mais notáveis dos LLMs é sua capacidade de realizar generalização em tarefas para as quais não foram explicitamente programados. Isso ocorre porque, durante o treinamento, os modelos aprendem padrões sintáticos, semânticos e pragmáticos da linguagem, o que os capacita a inferir intenções, completar sentenças e até mesmo gerar respostas inéditas com base em instruções ambíguas ou vagas \cite{liu2024hallucinations, fan2023llmsw}.

Nesse sentido, no contexto da engenharia de software, os LLMs têm sido amplamente empregados para atividades como compreensão de requisitos, geração de código, documentação automatizada e verificação semântica. Tais aplicações se beneficiam da habilidade dos modelos em transitar entre linguagem natural e linguagem de programação, estabelecendo uma ponte entre a comunicação humana e a lógica computacional. Conforme apontado por \citeonline{jelodar2025}, o uso de LLMs em tarefas de análise de código tem-se mostrado promissor por sua capacidade de capturar estruturas semânticas profundas e apoiar a automação de tarefas críticas em ambientes de desenvolvimento.

Em suma, os LLMs representam um avanço significativo na interseção entre inteligência artificial e linguagem, oferecendo uma base tecnológica versátil para a construção de sistemas inteligentes capazes de compreender e manipular texto com sofisticação crescente.

\section{\textit{Arquitetura Transformer}}

O avanço dos LLMs está intrinsecamente ligado à evolução de suas arquiteturas. Entre as diversas estruturas propostas, a arquitetura \textit{Transformer} representa um marco fundamental na história do processamento de linguagem natural, estabelecendo as bases dos modelos contemporâneos mais poderosos, como BERT, GPT e LaMDA.

A arquitetura Transformer foi projetada para resolver tarefas de transdução de sequência, como a tradução automática, de maneira mais eficiente que os modelos recorrentes anteriores, como RNNs e LSTMs. O elemento central do Transformer é o mecanismo de atenção, especialmente a \textit{self-attention}, que permite ao modelo capturar dependências contextuais entre quaisquer partes da entrada, independentemente de sua distância posicional, superando limitações comuns às redes recorrentes no aprendizado de relações de longo alcance no texto \cite{vaswani2017}.

A arquitetura original é composta por dois componentes principais: o codificador (\textit{encoder}) e o decodificador (\textit{decoder}). Ambos consistem em múltiplas camadas empilhadas, sendo que cada camada do codificador contém dois subcomponentes: um mecanismo de atenção multi-cabeça (\textit{multi-head attention}) e uma rede \textit{feedforward} totalmente conectada. O decodificador, por sua vez, incorpora um terceiro mecanismo de atenção, chamado \textit{encoder-decoder attention}, que permite integrar as representações aprendidas pelo codificador na geração das saídas \cite{ankit2024transformer}.  Essa estrutura pode ser visualizada na Figura~\ref{fig:transformer}, que ilustra a disposição dos principais componentes da arquitetura \textit{Transformer} proposta por \citeonline{vaswani2017}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{imgs/transformer.png}
    \caption{Arquitetura do modelo \textit{Transformer} \cite{vaswani2017}}
    \label{fig:transformer}
\end{figure}


Um dos aspectos mais inovadores do \textit{Transformer} é a sua natureza altamente paralelizável, possibilitando ganhos significativos em desempenho computacional. Diferentemente das RNNs, que processam dados de forma sequencial, o Transformer permite o processamento simultâneo de todos os elementos da sequência de entrada, aproveitando de forma mais eficiente a arquitetura paralela das GPUs modernas \cite{ankit2024transformer}.

Para representar a ordem dos \textit{tokens}, já que o modelo não possui uma estrutura sequencial intrínseca, são utilizados codificadores posicionais (\textit{positional encodings}), geralmente baseados em funções senoidais e cossenoidais de frequências variadas, os quais são somados aos vetores de \textit{embeddings} dos \textit{tokens} como diz a pesquisa de \citeonline{vaswani2017}. Essa codificação permite ao modelo inferir relações posicionais relativas entre os elementos da sequência.

Cada mecanismo de atenção da arquitetura é implementado por meio da chamada \textit{Scaled Dot-Product Attention}, onde o vetor de consulta (\textit{query}) é comparado com os vetores de chave (\textit{key}), e os resultados ponderam os vetores de valor (\textit{value}) para determinar a relevância contextual de cada \textit{token}. Esse processo é repetido em múltiplas cabeças de atenção (\textit{multi-head}), permitindo que o modelo aprenda diferentes aspectos das relações semânticas em paralelo \cite{ankit2024transformer, raschka2025bigllm}.

A partir da arquitetura original, diversas variantes e otimizações foram propostas. Modelos como GPT, BERT e T5 introduziram variações no uso do codificador, decodificador ou ambos, bem como aprimoramentos em camadas de normalização, mecanismos de atenção e escalabilidade. Além disso, técnicas como o uso de \textit{Mixture-of-Experts} (MoE), \textit{Grouped Query Attention} (GQA) e mecanismos de janela deslizante (\textit{sliding window attention}) foram desenvolvidas para aumentar a capacidade dos modelos e, ao mesmo tempo, reduzir os custos computacionais durante a inferência \citeonline{raschka2025bigllm}.

Esses avanços demonstram que, embora a estrutura base do \textit{Transformer} permaneça amplamente utilizada, sua flexibilidade arquitetônica possibilitou diversas inovações. Atualmente, a arquitetura \textit{Transformer} constitui a espinha dorsal da maioria dos LLMs de alto desempenho, permitindo não apenas avanços em tarefas linguísticas, mas também sua aplicação em domínios multimodais, como imagens, vídeos e interações conversacionais \cite{ankit2024transformer, raschka2025bigllm}.


\section{Janela de Contexto}

A janela de contexto de um modelo de linguagem representa a quantidade de informações textuais que o modelo é capaz de processar simultaneamente. Trata-se de um parâmetro crucial para tarefas que envolvem raciocínio sobre longos documentos, como verificação de requisitos em código-fonte, sumarização e busca semântica. O tamanho e o funcionamento da janela de contexto têm implicações diretas na eficácia e nas limitações dos modelos de linguagem natural, particularmente os baseados na arquitetura \textit{Transformer}.

O modelo \textit{Transformer}, proposto por \citeonline{vaswani2017}, introduziu o mecanismo de autoatenção como forma de modelar dependências globais entre \textit{tokens} sem recorrer a estruturas sequenciais como RNNs. Cada \textit{token} pode se conectar com qualquer outro \textit{token} da sequência por meio de uma matriz de atenção, o que implica em uma complexidade quadrática em relação ao tamanho da entrada. Embora isso permita modelar dependências de longo alcance com eficiência teórica, o custo computacional impõe limites práticos ao comprimento da janela de contexto suportada.

Para contornar essa limitação, estratégias como o \textit{chunking} (divisão de textos em segmentos menores) passaram a ser adotadas em sistemas de Recuperação Aumentada por Geração (RAG). Contudo, o \textit{chunking} tradicional, no qual os trechos são vetorizados separadamente, pode causar a perda de contexto semântico. Para enfrentar esse desafio, \citeonline{gunther2025} propuseram o método \textit{Late Chunking}, que primeiro vetoriza todo o documento com modelos de contexto longo e, só depois, aplica o agrupamento para o cálculo das representações vetoriais dos \textit{chunks}. Isso permite que os vetores reflitam a semântica global do documento, superando a limitação contextual dos métodos tradicionais.

Além disso, \citeonline{jin2024} propuseram o método \textit{SelfExtend}, que demonstra que modelos com codificações posicionais relativas, como RoPE, possuem capacidades inatas para processar sequências mais longas do que aquelas observadas durante o treinamento. A técnica consiste em modificar a atenção de forma que posições relativas fora da distribuição original sejam mapeadas para valores já observados, estendendo efetivamente a janela de contexto sem necessidade de \textit{fine-tuning}. Isto é especialmente relevante para tarefas que requerem consistência contextual em documentos extensos, como a análise de requisitos de software dispersos em arquivos distintos.

Ambas as abordagens mostram que a eficácia do modelo diante de textos longos depende não apenas do tamanho da janela de contexto, mas também da forma como a informação contextual é preservada e utilizada na geração dos \textit{embeddings}. Neste sentido, métodos como \textit{Late Chunking} e \textit{SelfExtend} tornam-se particularmente promissores para aplicações em que a rastreabilidade e a fidelidade semântica ao longo do texto são essenciais.

Por fim, destaca-se que o aproveitamento eficiente da janela de contexto é diretamente influenciado pelo design da arquitetura, pelas estratégias de vetorização e pela engenharia de \textit{prompts}. Assim, o estudo da janela de contexto é um componente fundamental para compreender os limites e as possibilidades dos modelos de linguagem em tarefas que envolvem grandes volumes de informação textual.

\section{Funcionamento e Parâmetros dos Modelos de Linguagem}

Os LLMs funcionam por meio de um processo dividido em duas fases: treinamento e geração. Na fase de treinamento, o modelo é exposto a grandes volumes de dados textuais e ajusta internamente seus parâmetros para aprender padrões linguísticos, sintáticos e semânticos. O número de parâmetros varia conforme o modelo, podendo atingir centenas de bilhões ou até trilhões, como é o caso do GPT-4, que apresenta arquitetura do tipo \textit{mixture-of-experts} com cerca de 1,8 trilhão de parâmetros \citeonline{openai2023}. Esses parâmetros são responsáveis por representar o conhecimento aprendido e são fixados após o treinamento. Na fase de inferência, por sua vez, o comportamento do modelo passa a ser governado por hiperparâmetros que controlam como as respostas são geradas a partir desse conhecimento prévio.

Entre esses hiperparâmetros, destaca-se a \textit{temperature}, que regula o grau de aleatoriedade da geração textual. Em termos práticos, valores baixos de \textit{temperature} (próximos de 0) resultam em respostas mais determinísticas, favorecendo a exatidão e a repetição de padrões aprendidos; por outro lado, valores mais altos (acima de 1.0) promovem maior diversidade e imprevisibilidade na escolha dos \textit{tokens} subsequentes. A configuração apropriada desse parâmetro é especialmente relevante em contextos técnicos, como a análise de requisitos de software, nos quais a fidelidade semântica e a consistência textual são essenciais como destacado por \citeonline{renze2024}.

Outros parâmetros amplamente utilizados incluem o \textit{top-k} e o \textit{top-p}. O \textit{top-k} restringe a escolha do próximo \textit{token} aos `k` \textit{tokens} mais prováveis, limitando o espaço de amostragem e eliminando opções de baixa probabilidade, o que favorece a coesão em tarefas com vocabulário mais técnico ou padronizado \citeonline{holtzman2020}. O \textit{top-p}, também conhecido como \textit{nucleus sampling}, adota uma abordagem dinâmica ao selecionar o menor conjunto de \textit{tokens} cuja soma de probabilidades atinja um limite cumulativo (geralmente entre 0,8 e 0,95), permitindo um equilíbrio entre variedade e controle estatístico da geração destaca  \citeonline{fan2018}.

Além disso, a \citeonline{openai2023} traz que penalidades de frequência e de presença são utilizadas para mitigar repetições, incentivando o uso de vocabulário mais diverso em textos extensos. Estas penalidades reduzem a probabilidade de reaparecimento de \textit{tokens} já utilizados, sendo particularmente úteis em aplicações que envolvem geração de documentação técnica, explicações ou justificativas complexas.

A interação entre esses hiperparâmetros permite a adaptação do comportamento do modelo às necessidades específicas de diferentes tarefas. Por exemplo, na verificação automatizada de requisitos, recomenda-se utilizar \textit{temperature} baixa e \textit{top-p} moderado, a fim de priorizar a exatidão e reduzir alucinações. Já em tarefas de geração descritiva ou criativa, como a escrita de histórias de usuários ou a geração de testes automatizados com descrições narrativas, configurações mais permissivas favorecem a variabilidade sem comprometer a utilidade, explica\citeonline{raffel2020}.

Embora tradicionalmente se associe a \textit{temperature} ao controle da “criatividade” de um modelo, estudos como o de \citeonline{nguyen2025} indicam que sua influência é limitada em comparação com outros fatores, como a estrutura da tarefa ou o tipo de \textit{fine-tuning} aplicado. Isso reforça a necessidade de compreender esses parâmetros como elementos que interagem entre si, exigindo ajustes experimentais cuidadosos conforme o domínio de aplicação.

Dessa forma, os parâmetros de decodificação exercem papel central na fase de inferência dos modelos de linguagem, moldando o estilo, a coerência e a adequação da saída gerada. No contexto de engenharia de software, em especial na análise de conformidade de requisitos, sua correta configuração é essencial para garantir não apenas a relevância da resposta, mas também a rastreabilidade semântica e a consistência técnica com os artefatos analisados.

\section{Embedding, Chunk e Vetorização}

O uso de \textit{embeddings} vetoriais tem se consolidado como uma abordagem essencial no processamento de linguagem natural (PLN), especialmente no contexto de LLMs. A técnica consiste na transformação de dados textuais não estruturados em vetores de alta dimensionalidade que capturam semântica e contexto, permitindo que sistemas computacionais realizem tarefas como busca semântica, recuperação de informações e verificação de similaridade com maior eficiência \citeonline{lema2025}.

O processo de vetorização normalmente se inicia com a fragmentação do conteúdo textual em segmentos menores, denominados \textit{chunks}. Esses \textit{chunks} são projetados para respeitar os limites da janela de contexto do modelo, otimizando a capacidade de raciocínio e minimizando a perda de informação semântica. A definição do tamanho do \textit{chunk} deve equilibrar a granularidade da análise e a eficiência da indexação, sendo comum a adoção de janelas de 512 a 1024 \textit{tokens} em aplicações práticas \citeonline{muennighoff2022}.

Uma vez definidos, os \textit{chunks} são processados por modelos de \textit{embedding}, os quais produzem representações vetoriais que preservam relações semânticas e sintáticas entre os textos originais. No caso do SGPT (\textit{Sentence Generative Pre-trained Transformer}), por exemplo, é aplicada uma técnica de \textit{weighted mean pooling} sobre os estados ocultos dos \textit{tokens}, ponderando a influência de \textit{tokens} mais à frente na sequência, uma vez que o modelo é baseado em atenção causal \citeonline{muennighoff2022}. Esta abordagem demonstrou superioridade em tarefas de busca semântica simétrica e assimétrica, com ganhos de desempenho notáveis quando comparados a \textit{embeddings} tradicionais.

Após a geração dos \textit{embeddings}, esses vetores são armazenados em bases de dados vetoriais (\textit{Vector Databases} – VDBs), que viabilizam operações de similaridade por meio de técnicas de busca aproximada ao vizinho mais próximo (\textit{Approximate Nearest Neighbor Search} – ANNS). Tais técnicas utilizam estruturas como índices hierárquicos, árvores k-d, grafos navegáveis e particionamentos baseados em quantização para permitir a recuperação eficiente mesmo em cenários de altíssima dimensionalidade \citeonline{lema2025}. Essa arquitetura é particularmente relevante para aplicações que requerem resposta em tempo real ou processamento em larga escala, como nos sistemas de Recuperação Aumentada por Geração (RAG) empregados em soluções corporativas e agentes autônomos.

A eficácia do processo de vetorização não depende apenas do modelo utilizado para gerar os \textit{embeddings}, mas também de boas práticas no pré-processamento textual, como a remoção de ruídos e a padronização da linguagem. Além disso, a forma como os \textit{chunks} são segmentados influencia diretamente na manutenção do contexto e da coerência semântica ao longo dos vetores gerados \citeonline{muennighoff2022}. Outro aspecto importante é a aplicação de técnicas de normalização nos vetores, como a normalização L2, que garante que as métricas de similaridade permaneçam matematicamente consistentes e comparáveis em grandes volumes de dados \citeonline{lema2025}.

A integração dessas tecnologias tem permitido o desenvolvimento de sistemas inteligentes capazes de realizar busca semântica de alta precisão, detecção de duplicidade textual, verificação de conformidade entre requisitos e código-fonte, bem como sumarização e rastreabilidade de informações em \textit{pipelines} automatizados de engenharia de software.

\section{Engenharia de Prompt e Protocolo MCP}

A engenharia de \textit{prompt} tem emergido como uma prática central na interação com LLMs, sendo especialmente relevante em contextos empresariais e acadêmicos. Essa técnica consiste na formulação de instruções textuais com o objetivo de elicitar comportamentos específicos dos modelos, como sumarização, classificação, geração de código e análise de documentos. \citeonline{desmond2024} destacam que, embora o uso de linguagem natural sugira uma interação intuitiva, a construção de \textit{prompts} eficazes exige conhecimento técnico, compreensão do funcionamento interno dos modelos e uma abordagem iterativa.

Estudos conduzidos por \citeonline{kin2023} apontam que usuários frequentemente cometem erros conceituais ao tentar interagir com LLMs, o que evidencia a necessidade de ferramentas e metodologias que auxiliem na criação de \textit{prompts} mais robustos. Complementarmente, \citeonline{braun2024} propõem uma taxonomia dos componentes do \textit{prompt}, destacando a importância do refinamento das instruções e da adaptação do conteúdo contextual.

Pesquisas empíricas demonstram que usuários experientes costumam ajustar elementos do \textit{prompt} como a instrução da tarefa, o contexto fornecido e o formato esperado da resposta. De acordo com \citeonline{desmond2024}, as alterações mais frequentes incluem modificações semânticas, adição de exemplos e reestruturações de formato. Além disso, estratégias avançadas têm se mostrado eficazes em tarefas complexas, como o \textit{few-shot prompting}, que consiste em fornecer alguns exemplos da tarefa desejada para guiar o modelo, e o \textit{chain-of-thought}. Esta última técnica é definida pelo "encadeamento de raciocínio" no \textit{prompt}, o que, segundo \citeonline{wei2022}, auxilia na execução de tarefas que requerem múltiplas etapas cognitivas. Em outra abordagem, \citeonline{melamed2023} tratam o design de \textit{prompts} como um problema inverso, propondo modelos que otimizam automaticamente sua estrutura para maximizar a performance.

A complexidade da engenharia de \textit{prompt} aumenta quando se busca garantir rastreabilidade entre os requisitos expressos em linguagem natural e os artefatos de software correspondentes. Nesse contexto, destaca-se o uso do \textit{Model Context Protocol} (MCP) como um mecanismo padronizado que organiza a comunicação entre modelos de linguagem e sistemas externos. O MCP opera em uma arquitetura cliente-servidor, onde aplicações de IA (\textit{hosts}) se conectam a servidores MCP especializados para obter dados contextuais relevantes. Esses servidores podem disponibilizar ferramentas, recursos e \textit{prompts} estruturados, que são acessados dinamicamente pelos modelos durante a execução de tarefas.

O protocolo é dividido em duas camadas principais: a camada de dados, baseada em JSON-RPC 2.0, que define como tarefas, respostas e recursos são trocados; e a camada de transporte, responsável pela conexão e autenticação, que pode funcionar via STDIO para servidores locais ou HTTP para servidores remotos \citeonline{mcp2024}.

Ao integrar a engenharia de \textit{prompt} com o MCP, é possível criar fluxos interativos e auditáveis nos quais os modelos obtêm contexto em tempo real, executam ferramentas externas ou acessam bases de conhecimento estruturadas com alta rastreabilidade. Essa abordagem fortalece a confiabilidade das LLMs em tarefas críticas, como análise de requisitos de software, verificação de conformidade e geração automatizada de artefatos técnicos.


\section{Agentes de Inteligência Artificial}


Agentes de Inteligência Artificial são entidades autônomas de software projetadas para executar tarefas específicas de forma adaptativa, racional e contínua. Fundamentados em ciclos de percepção, raciocínio e ação, esses agentes analisam entradas do ambiente (digitais ou físicos), tomam decisões com base em regras, heurísticas ou modelos treinados, e atuam para atingir metas estabelecidas. Diferentemente de assistentes baseados apenas em prompts ou scripts determinísticos, os agentes de IA modernos possuem certo grau de autonomia operacional, podendo interagir com APIs, sistemas de arquivos, modelos de linguagem e bancos de dados (IBM, 2024; AWS, 2024).

Com o avanço dos modelos fundacionais, especialmente os LLMs, esses agentes passaram a incorporar mecanismos de raciocínio natural, recuperação semântica e execução contextualizada de comandos. Isso os torna aptos a realizar tarefas como responder perguntas, sintetizar documentos, realizar buscas, chamar funções externas, interpretar imagens, gerar relatórios ou interagir com sistemas complexos de forma autônoma (Sapkota et al., 2025). Aplicações práticas incluem assistentes corporativos inteligentes, bots de atendimento, análise automatizada de requisitos de software e agentes robóticos embarcados. Empresas como AWS, Google, IBM e Microsoft já adotam arquiteturas baseadas em agentes para orquestração de tarefas entre serviços internos e externos.

A IBM (2024) propõe uma arquitetura modular baseada em três elementos principais: um plano de raciocínio e tomada de decisão; um componente de memória para estado e contexto; e um executor que interage com ferramentas, sistemas e APIs. Essa estrutura permite que o agente seja continuamente invocado por eventos, mantenha persistência sobre tarefas anteriores e adapte seu comportamento a partir do histórico. Além disso, agentes podem ser organizados em sistemas cooperativos, uma abordagem conhecida como Agentic AI  em que múltiplos agentes especializados trabalham de forma coordenada para resolver problemas complexos, com divisão de tarefas, comunicação entre agentes e reuso de memória compartilhada (Sapkota et al., 2025).

O conceito de Agentic AI amplia a atuação dos agentes convencionais ao introduzir estruturas colaborativas compostas por unidades autônomas com papéis distintos. Tais agentes são capazes de operar em camadas distintas, como planejadores, executores ou verificadores, interagindo entre si para atingir metas coletivas de forma eficaz. Ferramentas como AutoGen, CrewAI e LangGraph já exploram esse paradigma, permitindo a criação de pipelines autônomos, fluxos de raciocínio distribuído e agentes capazes de coordenar ações em ambientes híbridos físicos-digitais com alto grau de adaptabilidade (Sapkota et al., 2025).
